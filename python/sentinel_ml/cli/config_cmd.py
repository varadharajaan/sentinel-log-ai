"""
Configuration management commands.

Provides commands for generating, validating, and managing
configuration files.

Commands:
- config init: Generate sample configuration file
- config validate: Validate existing configuration
- config show: Display current configuration
"""

from __future__ import annotations

from pathlib import Path
from typing import TYPE_CHECKING, Any

import yaml

from sentinel_ml.config import (
    ClusteringConfig,
    Config,
    EmbeddingConfig,
    LoggingConfig,
    LLMConfig,
    NoveltyConfig,
    ServerConfig,
    VectorStoreConfig,
)
from sentinel_ml.logging import get_logger

if TYPE_CHECKING:
    pass

logger = get_logger(__name__)


# Default configuration as YAML template
DEFAULT_CONFIG_YAML = """\
# Sentinel Log AI Configuration
# Generated by: sentinel-ml config init
# Documentation: https://github.com/sentinel-log-ai/sentinel-log-ai#configuration

# =============================================================================
# Server Configuration
# =============================================================================
server:
  # gRPC server host and port
  host: "0.0.0.0"
  port: 50051
  
  # Maximum concurrent workers
  max_workers: 10
  
  # Request timeout in seconds
  timeout: 30.0
  
  # Enable reflection for grpcurl/debugging
  reflection: true

# =============================================================================
# Logging Configuration
# =============================================================================
logging:
  # Log level: DEBUG, INFO, WARNING, ERROR
  level: "INFO"
  
  # Output format: json, text
  format: "json"
  
  # Log file path (optional, logs to stderr if not set)
  # file: "logs/sentinel-ml.jsonl"
  
  # Enable structured logging
  structured: true

# =============================================================================
# Embedding Configuration
# =============================================================================
embedding:
  # Model name (sentence-transformers model)
  # Options: all-MiniLM-L6-v2, all-mpnet-base-v2, paraphrase-MiniLM-L6-v2
  model_name: "all-MiniLM-L6-v2"
  
  # Embedding dimension (must match model)
  dimension: 384
  
  # Batch size for embedding generation
  batch_size: 32
  
  # Enable caching for repeated embeddings
  cache_enabled: true
  cache_size: 10000
  
  # Device: cpu, cuda, mps (auto-detected if not set)
  # device: "cpu"
  
  # Use mock embeddings (for testing)
  use_mock: false

# =============================================================================
# Vector Store Configuration
# =============================================================================
vector_store:
  # Index type: flat, ivf, hnsw
  # - flat: Exact search, best for < 10k vectors
  # - ivf: Inverted file index, good balance
  # - hnsw: Hierarchical NSW, fast approximate search
  index_type: "flat"
  
  # Embedding dimension (must match embedding.dimension)
  dimension: 384
  
  # Metric: cosine, l2, inner_product
  metric: "cosine"
  
  # IVF-specific settings
  ivf_nlist: 100
  ivf_nprobe: 10
  
  # HNSW-specific settings
  hnsw_m: 32
  hnsw_ef_construction: 200
  hnsw_ef_search: 64
  
  # Persistence path (optional)
  # persist_path: "data/vector_store"
  
  # Use mock store (for testing)
  use_mock: false

# =============================================================================
# Clustering Configuration
# =============================================================================
clustering:
  # Algorithm: hdbscan, kmeans, dbscan
  algorithm: "hdbscan"
  
  # HDBSCAN parameters
  min_cluster_size: 5
  min_samples: 3
  cluster_selection_epsilon: 0.0
  cluster_selection_method: "eom"  # eom or leaf
  
  # K-Means parameters (if algorithm: kmeans)
  n_clusters: 10
  
  # DBSCAN parameters (if algorithm: dbscan)
  eps: 0.5
  
  # Metric for distance calculation
  metric: "euclidean"
  
  # Use mock clustering (for testing)
  use_mock: false

# =============================================================================
# Novelty Detection Configuration
# =============================================================================
novelty:
  # Algorithm: knn, lof, isolation_forest
  algorithm: "knn"
  
  # Number of neighbors for k-NN
  k_neighbors: 5
  
  # Threshold for novelty classification (0.0 - 1.0)
  novelty_threshold: 0.7
  
  # Minimum samples to fit detector
  min_samples_fit: 10
  
  # LOF-specific (if algorithm: lof)
  lof_contamination: 0.1
  
  # Isolation Forest (if algorithm: isolation_forest)
  if_n_estimators: 100
  if_contamination: 0.1
  
  # Use mock detector (for testing)
  use_mock: false

# =============================================================================
# LLM Configuration
# =============================================================================
llm:
  # Provider: ollama, openai, mock
  provider: "ollama"
  
  # Model name
  model: "llama3.2"
  
  # Base URL for Ollama
  base_url: "http://localhost:11434"
  
  # Request timeout in seconds
  timeout: 120
  
  # Maximum retries on failure
  max_retries: 3
  
  # Temperature for generation (0.0 - 1.0)
  temperature: 0.1
  
  # Use mock LLM (for testing)
  use_mock: false

# =============================================================================
# CLI Configuration
# =============================================================================
cli:
  # Theme: dark, light, minimal, colorblind, none
  theme: "dark"
  
  # Output format: text, json, table
  output_format: "text"
  
  # Enable colors (auto-detected if terminal supports)
  colors: true
  
  # Maximum output width
  max_width: 120
  
  # Show timestamps in output
  timestamps: true
  
  # Verbose output
  verbose: false

# =============================================================================
# Profile Configuration (Performance)
# =============================================================================
profile:
  # Enable profiling
  enabled: false
  
  # Minimum duration to report (ms)
  threshold_ms: 1.0
  
  # Output file (optional)
  # output_file: "profile.json"
"""


def generate_config(
    output_path: str | Path | None = None,
    *,
    include_comments: bool = True,
    minimal: bool = False,
) -> str:
    """
    Generate configuration file content.

    Args:
        output_path: Path to write config (None = return string only).
        include_comments: Include explanatory comments.
        minimal: Generate minimal config with only required fields.

    Returns:
        Configuration file content.
    """
    if minimal:
        content = _generate_minimal_config()
    elif include_comments:
        content = DEFAULT_CONFIG_YAML
    else:
        content = _generate_uncommented_config()

    if output_path:
        path = Path(output_path)
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(content, encoding="utf-8")
        logger.info("config_generated", path=str(path))

    return content


def _generate_minimal_config() -> str:
    """Generate minimal configuration."""
    config = {
        "server": {"port": 50051},
        "logging": {"level": "INFO"},
        "embedding": {"model_name": "all-MiniLM-L6-v2"},
        "clustering": {"algorithm": "hdbscan"},
        "llm": {"provider": "ollama", "model": "llama3.2"},
    }
    return yaml.dump(config, default_flow_style=False, sort_keys=False)


def _generate_uncommented_config() -> str:
    """Generate config without comments."""
    # Parse default config and remove comments
    lines = []
    for line in DEFAULT_CONFIG_YAML.split("\n"):
        stripped = line.strip()
        if stripped.startswith("#"):
            continue
        if "#" in line and not line.strip().startswith("#"):
            # Remove inline comments
            line = line.split("#")[0].rstrip()
        if line.strip():
            lines.append(line)
    return "\n".join(lines)


def validate_config(config_path: str | Path) -> tuple[bool, list[str]]:
    """
    Validate a configuration file.

    Args:
        config_path: Path to configuration file.

    Returns:
        Tuple of (is_valid, list of error messages).
    """
    errors: list[str] = []
    path = Path(config_path)

    if not path.exists():
        return False, [f"Configuration file not found: {path}"]

    try:
        content = path.read_text(encoding="utf-8")
        data = yaml.safe_load(content)
    except yaml.YAMLError as e:
        return False, [f"Invalid YAML syntax: {e}"]

    if not isinstance(data, dict):
        return False, ["Configuration must be a dictionary"]

    # Validate each section
    if "server" in data:
        errors.extend(_validate_server_config(data["server"]))

    if "embedding" in data:
        errors.extend(_validate_embedding_config(data["embedding"]))

    if "clustering" in data:
        errors.extend(_validate_clustering_config(data["clustering"]))

    if "llm" in data:
        errors.extend(_validate_llm_config(data["llm"]))

    is_valid = len(errors) == 0

    if is_valid:
        logger.info("config_validated", path=str(path))
    else:
        logger.warning("config_validation_failed", path=str(path), errors=errors)

    return is_valid, errors


def _validate_server_config(config: dict[str, Any]) -> list[str]:
    """Validate server configuration section."""
    errors = []

    if "port" in config:
        port = config["port"]
        if not isinstance(port, int) or port < 1 or port > 65535:
            errors.append(f"server.port must be integer 1-65535, got: {port}")

    if "max_workers" in config:
        workers = config["max_workers"]
        if not isinstance(workers, int) or workers < 1:
            errors.append(f"server.max_workers must be positive integer, got: {workers}")

    return errors


def _validate_embedding_config(config: dict[str, Any]) -> list[str]:
    """Validate embedding configuration section."""
    errors = []

    if "dimension" in config:
        dim = config["dimension"]
        if not isinstance(dim, int) or dim < 1:
            errors.append(f"embedding.dimension must be positive integer, got: {dim}")

    if "batch_size" in config:
        batch = config["batch_size"]
        if not isinstance(batch, int) or batch < 1:
            errors.append(f"embedding.batch_size must be positive integer, got: {batch}")

    return errors


def _validate_clustering_config(config: dict[str, Any]) -> list[str]:
    """Validate clustering configuration section."""
    errors = []

    valid_algorithms = {"hdbscan", "kmeans", "dbscan"}
    if "algorithm" in config:
        algo = config["algorithm"]
        if algo not in valid_algorithms:
            errors.append(f"clustering.algorithm must be one of {valid_algorithms}, got: {algo}")

    if "min_cluster_size" in config:
        size = config["min_cluster_size"]
        if not isinstance(size, int) or size < 2:
            errors.append(f"clustering.min_cluster_size must be >= 2, got: {size}")

    return errors


def _validate_llm_config(config: dict[str, Any]) -> list[str]:
    """Validate LLM configuration section."""
    errors = []

    valid_providers = {"ollama", "openai", "mock"}
    if "provider" in config:
        provider = config["provider"]
        if provider not in valid_providers:
            errors.append(f"llm.provider must be one of {valid_providers}, got: {provider}")

    if "temperature" in config:
        temp = config["temperature"]
        if not isinstance(temp, (int, float)) or temp < 0 or temp > 2:
            errors.append(f"llm.temperature must be 0.0-2.0, got: {temp}")

    return errors


def load_config(config_path: str | Path) -> Config:
    """
    Load and parse configuration file.

    Args:
        config_path: Path to configuration file.

    Returns:
        Parsed Config object.

    Raises:
        ValueError: If configuration is invalid.
    """
    path = Path(config_path)

    # Validate first
    is_valid, errors = validate_config(path)
    if not is_valid:
        raise ValueError(f"Invalid configuration: {'; '.join(errors)}")

    content = path.read_text(encoding="utf-8")
    data = yaml.safe_load(content)

    # Build config objects
    config = Config(
        server=ServerConfig(**data.get("server", {})) if "server" in data else ServerConfig(),
        logging=LoggingConfig(**data.get("logging", {})) if "logging" in data else LoggingConfig(),
        embedding=EmbeddingConfig(**data.get("embedding", {}))
        if "embedding" in data
        else EmbeddingConfig(),
        vector_store=VectorStoreConfig(**data.get("vector_store", {}))
        if "vector_store" in data
        else VectorStoreConfig(),
        clustering=ClusteringConfig(**data.get("clustering", {}))
        if "clustering" in data
        else ClusteringConfig(),
        novelty=NoveltyConfig(**data.get("novelty", {})) if "novelty" in data else NoveltyConfig(),
        llm=LLMConfig(**data.get("llm", {})) if "llm" in data else LLMConfig(),
    )

    logger.info("config_loaded", path=str(path))
    return config


def show_config(config: Config) -> str:
    """
    Format configuration for display.

    Args:
        config: Configuration object.

    Returns:
        Formatted configuration string.
    """
    sections = []

    # Server
    sections.append("Server:")
    sections.append(f"  host: {config.server.host}")
    sections.append(f"  port: {config.server.port}")

    # Embedding
    sections.append("\nEmbedding:")
    sections.append(f"  model: {config.embedding.model_name}")
    sections.append(f"  dimension: {config.embedding.dimension}")

    # Clustering
    sections.append("\nClustering:")
    sections.append(f"  algorithm: {config.clustering.algorithm}")
    sections.append(f"  min_cluster_size: {config.clustering.min_cluster_size}")

    # Novelty
    sections.append("\nNovelty:")
    sections.append(f"  algorithm: {config.novelty.algorithm}")
    sections.append(f"  threshold: {config.novelty.novelty_threshold}")

    # LLM
    sections.append("\nLLM:")
    sections.append(f"  provider: {config.llm.provider}")
    sections.append(f"  model: {config.llm.model}")

    return "\n".join(sections)
